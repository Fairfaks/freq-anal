# -*- coding: utf-8 -*-
"""
–ß–∞—Å—Ç–æ—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø—Ä–æ–¥–≤–∏–≥–∞–µ–º–æ–≥–æ —Å–∞–π—Ç–∞ vs –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ + —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –∫–æ—Ä–ø—É—Å–æ–º + —ç–∫—Å–ø–æ—Ä—Ç –≤ Excel.
–î–æ–±–∞–≤–ª–µ–Ω –ª–∏—Å—Ç ¬´Comparison¬ª —Å —É—Å–ª–æ–≤–Ω—ã–º —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º.
–ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏:
    pip install requests beautifulsoup4 lxml nltk pymorphy2 pandas numpy tqdm openpyxl xlsxwriter
"""

import inspect, os, csv, requests, re, nltk, pymorphy2, random
import pandas as pd, numpy as np
from bs4 import BeautifulSoup
from collections import Counter
from tqdm import tqdm

# --- –ü–∞—Ç—á –¥–ª—è pymorphy2 –Ω–∞ Python ‚â•3.11 ---
def _getargspec(func):
    spec = inspect.getfullargspec(func)
    return spec.args, spec.varargs, spec.varkw, spec.defaults
inspect.getargspec = _getargspec

# --- –°—Ç–æ–ø-—Å–ª–æ–≤–∞ NLTK ---
nltk.download('stopwords', quiet=True)
from nltk.corpus import stopwords

def download_corpus_if_needed(script_dir: str):
    """–°–∫–∞—á–∏–≤–∞–µ–º HermitDave –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ russian_freq.csv"""
    hd_url = (
        'https://raw.githubusercontent.com/'
        'hermitdave/FrequencyWords/master/content/2016/ru/ru_full.txt'
    )
    print("‚è≥ –°–∫–∞—á–∏–≤–∞–µ–º HermitDave...")
    r = requests.get(hd_url, timeout=60)
    r.raise_for_status()
    lines = r.text.splitlines()
    out = os.path.join(script_dir, 'russian_freq.csv')
    with open(out, 'w', encoding='utf-8') as f:
        f.write('word,freq_corpus\n')
        for L in lines:
            p = L.split()
            if len(p) >= 2:
                f.write(f"{p[0]},{p[1]}\n")
    print("‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ ‚Üí", out)

def load_corpus(path: str) -> pd.DataFrame:
    """–ß–∏—Ç–∞–µ–º CSV –∫–æ—Ä–ø—É—Å–∞ –∏ —Å—á–∏—Ç–∞–µ–º –ø—Ä–æ—Ü–µ–Ω—Ç–Ω—É—é —á–∞—Å—Ç–æ—Ç—É"""
    with open(path, 'r', encoding='utf-8-sig') as f:
        sample = f.read(2048)
        delim = csv.Sniffer().sniff(sample,
                                    delimiters=[',',';','\t','|']).delimiter
    reader = pd.read_csv(path, sep=delim, engine='python',
                         encoding='utf-8-sig',
                         usecols=lambda c: c.strip() in ('word','freq_corpus'),
                         skipinitialspace=True, chunksize=100_000)
    chunks, total = [], 0
    for chunk in tqdm(reader, desc="  —á—Ç–µ–Ω–∏–µ –∫–æ—Ä–ø—É—Å–∞", unit="—Å—Ç—Ä–æ–∫"):
        chunk.columns = chunk.columns.str.strip()
        chunks.append(chunk)
        total += chunk['freq_corpus'].sum()
    df = pd.concat(chunks, ignore_index=True)
    df['percent_corpus'] = df['freq_corpus'] / total * 100
    return df.rename(columns={'word':'–õ–µ–º–º–∞'})[['–õ–µ–º–º–∞','percent_corpus']]

def get_page_text(session: requests.Session,
                  url: str,
                  proxy: str = None) -> str:
    """–°–∫–∞—á–∏–≤–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É, —É–¥–∞–ª—è–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ —Ç–µ–≥–∏ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —á–∏—Å—Ç—ã–π —Ç–µ–∫—Å—Ç"""
    headers = {
        'User-Agent': (
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '
            'AppleWebKit/537.36 (KHTML, like Gecko) '
            'Chrome/114.0.0.0 Safari/537.36'
        ),
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'ru-RU,ru;q=0.9',
        'Connection': 'keep-alive',
        'Referer': url,
    }
    kwargs = {}
    if proxy:
        kwargs['proxies'] = {'http': proxy, 'https': proxy}
    resp = session.get(url, headers=headers, timeout=30, **kwargs)
    resp.raise_for_status()
    soup = BeautifulSoup(resp.text, 'lxml')
    for tag in soup(['script','style','nav','header','footer','aside']):
        tag.decompose()
    return soup.body.get_text(separator=' ', strip=True) if soup.body else soup.get_text()

def tokenize_morph(text: str) -> list[dict]:
    """–†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Ç–æ–∫–µ–Ω—ã, –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º pymorphy2 –∏ —Ñ–∏–ª—å—Ç—Ä—É–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞"""
    tokens = re.findall(r'\b[–∞-—è—ë]+\b', text.lower())
    out = []
    for w in tqdm(tokens, desc="  –º–æ—Ä—Ñ–æ-—Ç–æ–∫–µ–Ω—ã", unit="—Ç–æ–∫–µ–Ω"):
        if w in STOP_RU or len(w) < 3:
            continue
        p = MORPH.parse(w)[0]
        out.append({'word': w,
                    'lemma': p.normal_form,
                    'pos': p.tag.POS or ''})
    return out

def build_target_freq(morph_data: list[dict],
                      top_n: int = 100) -> pd.DataFrame:
    """–°—á–∏—Ç–∞–µ–º –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ –∏ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ —á–∞—Å—Ç–æ—Ç—ã –¥–ª—è —Ç–æ–ø-N –ª–µ–º–º"""
    total = len(morph_data) or 1
    cnt = Counter(item['word'] for item in morph_data)
    rows = []
    for word, freq in cnt.most_common(top_n):
        info = next(item for item in morph_data if item['word'] == word)
        rows.append({
            '–°–ª–æ–≤–æ': word,
            '–õ–µ–º–º–∞': info['lemma'],
            '–ß–∞—Å—Ç—å —Ä–µ—á–∏': info['pos'],
            '–ß–∞—Å—Ç–æ—Ç–∞': freq,
            '%_target': round(freq / total * 100, 3)
        })
    return pd.DataFrame(rows)

if __name__ == '__main__':
    script_dir = os.path.dirname(os.path.abspath(__file__))

    # ========== –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è MorphAnalyzer –∏ —Å—Ç–æ–ø-—Å–ª–æ–≤ ==========
    MORPH = pymorphy2.MorphAnalyzer()
    STOP_RU = set(stopwords.words('russian'))

    # 1) URL –ø—Ä–æ–¥–≤–∏–≥–∞–µ–º–æ–≥–æ —Å–∞–π—Ç–∞
    main_url = input("–í–≤–µ–¥–∏—Ç–µ URL –ø—Ä–æ–¥–≤–∏–≥–∞–µ–º–æ–≥–æ —Å–∞–π—Ç–∞: ").strip()

    # 2) URL –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤
    raw = input("–í–≤–µ–¥–∏—Ç–µ URL –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é (–∏–ª–∏ Enter, –µ—Å–ª–∏ –±–µ–∑): ")
    comp_urls = [u.strip() for u in raw.split(',') if u.strip()]

    # 3) –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–æ–∫—Å–∏?
    use_proxy = input("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–æ–∫—Å–∏? (–¥–∞/–Ω–µ—Ç): ").strip().lower() in ('–¥–∞','–¥','yes','y')
    proxies: list[str] = []
    if use_proxy:
        print("–í–≤–µ–¥–∏—Ç–µ –ø—Ä–æ–∫—Å–∏ –ø–æ –æ–¥–Ω–æ–º—É (host:port –∏–ª–∏ http://user:pass@host:port). –ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ ‚Äî –∫–æ–Ω–µ—Ü –≤–≤–æ–¥–∞.")
        while True:
            p = input("–ü—Ä–æ–∫—Å–∏: ").strip()
            if not p:
                break
            proxies.append(p)
        if not proxies:
            print("‚ö†Ô∏è –ü—Ä–æ–∫—Å–∏ –Ω–µ –≤–≤–µ–¥–µ–Ω—ã ‚Äî –ø—Ä–æ–¥–æ–ª–∂–∏–º –±–µ–∑ –ø—Ä–æ–∫—Å–∏.")
            use_proxy = False

    # 4) –°–∫–∞—á–∞—Ç—å HermitDave?
    if input("–°–∫–∞—á–∞—Ç—å –∫–æ—Ä–ø—É—Å HermitDave? (–¥–∞/–Ω–µ—Ç): ").strip().lower() in ('–¥–∞','–¥','yes','y'):
        download_corpus_if_needed(script_dir)

    # 5) –í—ã–±–æ—Ä CSV-–∫–æ—Ä–ø—É—Å–∞ –≤ –ø–∞–ø–∫–µ —Å–æ —Å–∫—Ä–∏–ø—Ç–æ–º
    csvs = [f for f in os.listdir(script_dir) if f.lower().endswith('.csv')]
    print("\n–î–æ—Å—Ç—É–ø–Ω—ã–µ CSV –∫–æ—Ä–ø—É—Å–∞:")
    for i, fn in enumerate(csvs, 1):
        print(f"  {i}. {fn}")
    idx = int(input(f"–í—ã–±–µ—Ä–∏—Ç–µ –∫–æ—Ä–ø—É—Å (1‚Äì{len(csvs)}): ").strip()) - 1
    corpus_df = load_corpus(os.path.join(script_dir, csvs[idx]))

    # 6) Top-N
    try:
        top_n = int(input("–°–∫–æ–ª—å–∫–æ —Ç–æ–ø-—Å–ª–æ–≤ –±—Ä–∞—Ç—å? (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 100): ") or 100)
    except ValueError:
        top_n = 100

    # 7) –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–µ—Å—Å–∏–∏
    session = requests.Session()
    session.headers.update({'User-Agent': 'Mozilla/5.0 (–∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä)'})

    # 8) –ê–Ω–∞–ª–∏–∑ –≥–ª–∞–≤–Ω–æ–≥–æ —Å–∞–π—Ç–∞
    print(f"\nüîç –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–¥–≤–∏–≥–∞–µ–º–æ–≥–æ —Å–∞–π—Ç–∞: {main_url}")
    proxy = random.choice(proxies) if use_proxy else None
    main_text = get_page_text(session, main_url, proxy)
    main_morph = tokenize_morph(main_text)
    df_main = build_target_freq(main_morph, top_n)
    df_main['URL'] = main_url

    # 9) –ê–Ω–∞–ª–∏–∑ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤
    results_comp: dict[str, pd.DataFrame] = {}
    for url in comp_urls:
        print(f"\nüîç –ê–Ω–∞–ª–∏–∑ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞: {url}")
        proxy = random.choice(proxies) if use_proxy else None
        text = get_page_text(session, url, proxy)
        morph = tokenize_morph(text)
        df = build_target_freq(morph, top_n)
        df['URL'] = url
        results_comp[url] = df

    # 10) –û–±—ä–µ–¥–∏–Ω—è–µ–º –∏ —Å—á–∏—Ç–∞–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏
    all_df = pd.concat([df_main] + list(results_comp.values()), ignore_index=True)
    merged = all_df.merge(corpus_df, how='left', on='–õ–µ–º–º–∞')
    mz = merged['percent_corpus'][merged['percent_corpus']>0].min()
    merged['percent_corpus'] = merged['percent_corpus'].fillna(mz)
    merged['%_corpus']  = merged['percent_corpus'].round(3)
    merged['Ratio']     = (merged['%_target'] / merged['%_corpus']).round(3)
    merged['Log(Ratio)']= np.log(merged['Ratio']).round(3)
    comp_merged = merged[merged['URL'].isin(comp_urls)]

    # 11) –û–±—â–∏–µ –∏ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –ª–µ–º–º—ã
    all_urls = [main_url] + comp_urls
    n_pages = len(all_urls)

    common = (
        merged
        .groupby('–õ–µ–º–º–∞')['URL']
        .nunique()
        .reset_index(name='pages')
    )
    common = common[common['pages'] == n_pages].drop(columns='pages')

    unique: dict[str, pd.DataFrame] = {}
    for url in all_urls:
        df_url = merged[merged['URL'] == url]
        others = set(merged.loc[merged['URL'] != url, '–õ–µ–º–º–∞'])
        unique[url] = df_url[~df_url['–õ–µ–º–º–∞'].isin(others)].copy()

    # === 12) –°—Ä–µ–¥–Ω–∏–µ –ø–æ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞–º –∏ Comparison-DataFrame ===
    comp_stats = comp_merged.groupby('–õ–µ–º–º–∞').agg({
        '–ß–∞—Å—Ç–æ—Ç–∞': 'mean',
        '%_target': 'mean',
        'Ratio': 'mean'
    }).rename(columns={
        '–ß–∞—Å—Ç–æ—Ç–∞': '–ß–∞—Å—Ç–æ—Ç–∞_comp_avg',
        '%_target': '%_target_comp_avg',
        'Ratio': 'Ratio_comp_avg'
    }).reset_index()

    # –û–∫—Ä—É–≥–ª–µ–Ω–∏–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º —Å–ø–æ—Å–æ–±–æ–º
    comp_stats = comp_stats.round({
        '–ß–∞—Å—Ç–æ—Ç–∞_comp_avg': 2,
        '%_target_comp_avg': 3,
        'Ratio_comp_avg': 3
    })


    # === –í–°–¢–ê–í–ö–ê 1: –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–ª–æ–Ω–∫—É "–ß—Ç–æ –¥–µ–ª–∞—Ç—å" ===
    def decide_action(row):
        if row['%_target'] > row['%_target_comp_avg']:
            return '–£–º–µ–Ω—å—à–∏—Ç—å'
        elif row['%_target'] < row['%_target_comp_avg']:
            return '–î–æ–±–∞–≤–∏—Ç—å'
        else:
            return '–ù–∏—á–µ–≥–æ –Ω–µ –¥–µ–ª–∞—Ç—å'


    comp_df = df_main.merge(comp_stats, on='–õ–µ–º–º–∞', how='left')
    comp_df['–ß—Ç–æ –¥–µ–ª–∞—Ç—å'] = comp_df.apply(decide_action, axis=1)
    # === –ö–æ–Ω–µ—Ü –≤—Å—Ç–∞–≤–∫–∏ 1 ===

    # === 13) –≠–∫—Å–ø–æ—Ä—Ç –≤—Å–µ—Ö –ª–∏—Å—Ç–æ–≤ –≤ Excel ===
    out_xlsx = os.path.join(script_dir, 'multi_page_analysis.xlsx')
    with pd.ExcelWriter(out_xlsx, engine='xlsxwriter') as writer:
        merged.to_excel(writer, sheet_name='All', index=False)
        common.to_excel(writer, sheet_name='Common', index=False)
        # ‚Ä¶ –ª–∏—Å—Ç—ã Unique_‚Ä¶ ‚Ä¶
        comp_df.to_excel(writer, sheet_name='Comparison', index=False)

        # === –í–°–¢–ê–í–ö–ê 2: –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–π –ª–∏—Å—Ç "–ß—Ç–æ –¥–µ–ª–∞—Ç—å" ===
        comp_df[
            ['–°–ª–æ–≤–æ', '–õ–µ–º–º–∞', '%_target', '%_target_comp_avg', 'Ratio_comp_avg', '–ß—Ç–æ –¥–µ–ª–∞—Ç—å']
        ].to_excel(
            writer,
            sheet_name='–ß—Ç–æ –¥–µ–ª–∞—Ç—å',
            index=False
        )
        # === –ö–æ–Ω–µ—Ü –≤—Å—Ç–∞–≤–∫–∏ 2 ===

        # –£—Å–ª–æ–≤–Ω–æ–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –ª–∏—Å—Ç–∞ Comparison
        workbook = writer.book
        worksheet = writer.sheets['Comparison']
    # 13) –≠–∫—Å–ø–æ—Ä—Ç –≤—Å–µ—Ö –ª–∏—Å—Ç–æ–≤ –≤ Excel
    out_xlsx = os.path.join(script_dir, 'multi_page_analysis.xlsx')
    with pd.ExcelWriter(out_xlsx, engine='xlsxwriter') as writer:
        merged.to_excel(writer, sheet_name='All', index=False)
        common.to_excel(writer, sheet_name='Common', index=False)

        prefix = 'Unique_'
        max_len = 31 - len(prefix)
        for url, df_u in unique.items():
            raw = url.replace('https://','').replace('http://','').replace('/','_')
            sheet = prefix + raw[:max_len]
            df_u.to_excel(writer, sheet_name=sheet, index=False)

        comp_df.to_excel(writer, sheet_name='Comparison', index=False)

        # –£—Å–ª–æ–≤–Ω–æ–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è Comparison
        workbook  = writer.book
        worksheet = writer.sheets['Comparison']
        fmt_g = workbook.add_format({'bg_color':'#C6EFCE','font_color':'#006100'})
        fmt_r = workbook.add_format({'bg_color':'#FFC7CE','font_color':'#9C0006'})
        fmt_y = workbook.add_format({'bg_color':'#FFEB9C','font_color':'#9C6500'})
        nrows = len(comp_df) + 1

        # D vs E (–ß–∞—Å—Ç–æ—Ç–∞)
        worksheet.conditional_format(f'D2:D{nrows}', {
            'type':'formula','criteria':'=D2>E2','format':fmt_g})
        worksheet.conditional_format(f'D2:D{nrows}', {
            'type':'formula','criteria':'=D2<E2','format':fmt_r})
        worksheet.conditional_format(f'D2:D{nrows}', {
            'type':'formula','criteria':'=D2=E2','format':fmt_y})
        # F vs G (%_target)
        worksheet.conditional_format(f'F2:F{nrows}', {
            'type':'formula','criteria':'=F2>G2','format':fmt_g})
        worksheet.conditional_format(f'F2:F{nrows}', {
            'type':'formula','criteria':'=F2<G2','format':fmt_r})
        worksheet.conditional_format(f'F2:F{nrows}', {
            'type':'formula','criteria':'=F2=G2','format':fmt_y})

    print(f"\n‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à—ë–Ω. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ {out_xlsx}")
